{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "NBivgQ2kyZU_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Baca file JSON\n",
        "with open('/content/data_train (3).json', 'r') as file:\n",
        "    candidates = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jobreq = \"deskripsi kerja posisi lowong partisipasi prose bangun aplikasi sedang bangun usaha nama usaha harap mampu kerja tim responsibility buat modifikasi program tanggung jawab kelola program\""
      ],
      "metadata": {
        "id": "ic9iNG-nzBCM"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q googletrans==4.0.0-rc1\n",
        "!pip install -q googletrans==4.0.0-rc1 langdetect"
      ],
      "metadata": {
        "id": "91TQvDQtzJ0L"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "from langdetect import detect"
      ],
      "metadata": {
        "id": "-gcgtZYUzLrt"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_text(text):\n",
        "    translator = Translator()\n",
        "    try:\n",
        "        # Detect the language\n",
        "        lang = detect(text)\n",
        "        # Translate only if the text is in Indonesian\n",
        "        if lang == 'id':\n",
        "            translated = translator.translate(text, src='id', dest='en')\n",
        "            return translated.text\n",
        "        else:\n",
        "            # Return the original text if it's not in Indonesian\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        # If detection or translation fails, return the original text\n",
        "        return text"
      ],
      "metadata": {
        "id": "K3rZ2_PnzNnk"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "VaXhqMsNzPN7"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_duration(start_date, end_date):\n",
        "    if start_date.lower() in ['none', 'unknown']:\n",
        "        return \"Unknown duration\"\n",
        "\n",
        "    # Try parsing the start_date in different formats\n",
        "    try:\n",
        "        start_date_obj = datetime.strptime(start_date, \"%b %Y\")\n",
        "    except ValueError:\n",
        "        try:\n",
        "            start_date_obj = datetime.strptime(start_date, \"%Y\")\n",
        "        except ValueError:\n",
        "            return \"Unknown duration\"\n",
        "\n",
        "    if end_date.lower() == 'present':\n",
        "        end_date_obj = datetime.now()\n",
        "    elif end_date.lower() in ['none', 'unknown']:\n",
        "        return \"Unknown duration\"\n",
        "    else:\n",
        "        # Try parsing the end_date in different formats\n",
        "        try:\n",
        "            end_date_obj = datetime.strptime(end_date, \"%b %Y\")\n",
        "        except ValueError:\n",
        "            try:\n",
        "                end_date_obj = datetime.strptime(end_date, \"%Y\")\n",
        "            except ValueError:\n",
        "                return \"Unknown duration\"\n",
        "\n",
        "    # Calculate the difference between end_date and start_date\n",
        "    duration = end_date_obj - start_date_obj\n",
        "\n",
        "    # Extract years and months from the duration\n",
        "    years = duration.days // 365\n",
        "    months = (duration.days % 365) // 30\n",
        "\n",
        "    # Format the duration\n",
        "    if years >= 1:\n",
        "        if years == 1:\n",
        "            return \"+1 year\"\n",
        "        elif years == 2:\n",
        "            return \"+2 years\"\n",
        "        else:\n",
        "            return f\"+{years} years\"\n",
        "    else:\n",
        "        return f\"{months} months\""
      ],
      "metadata": {
        "id": "z4u5X6pZzQyS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to translate candidates' data\n",
        "def translate_candidate_data(candidate):\n",
        "    # Translate work experience descriptions\n",
        "    for experience in candidate.get(\"work_experience\", []):\n",
        "        # Translate job title\n",
        "        job_title = experience.get(\"job_title\", \"\")\n",
        "        if job_title:\n",
        "            translated_title = translate_text(\"\".join(job_title))\n",
        "            experience[\"job_title\"] = translated_title\n",
        "\n",
        "        # Translate job description\n",
        "        if experience.get(\"job_desc\"):\n",
        "            experience[\"job_desc\"] = [translate_text(desc) for desc in experience[\"job_desc\"]]\n",
        "\n",
        "        # Calculate duration\n",
        "        duration = calculate_duration(experience.get(\"start_date\", \"\"), experience.get(\"end_date\", \"\"))\n",
        "        experience[\"duration\"] = duration\n",
        "\n",
        "    # Translate education title and description if they exist\n",
        "    for education in candidate.get(\"education\", []):\n",
        "        if isinstance(education, dict):\n",
        "            # Translate education title\n",
        "            education_title = education.get(\"title\", \"\")\n",
        "            if education_title:\n",
        "                translated_education_title = translate_text(\"\".join(education_title))\n",
        "                education[\"title\"] = translated_education_title\n",
        "\n",
        "            # Translate education description\n",
        "            if education.get(\"description\"):\n",
        "                education[\"description\"] = translate_text(education[\"description\"])\n",
        "\n",
        "    # Translate languages\n",
        "    candidate[\"languages\"] = [translate_text(language) for language in candidate.get(\"languages\", [])]\n",
        "\n",
        "    # Translate skills\n",
        "    candidate[\"skills\"] = [translate_text(skill) for skill in candidate.get(\"skills\", [])]\n"
      ],
      "metadata": {
        "id": "vooUEy3JzScn"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for candidate in candidates:\n",
        "    translate_candidate_data(candidate)"
      ],
      "metadata": {
        "id": "RyRUPfm5zUA9"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the modified JSON for candidates\n",
        "print(json.dumps(candidates, indent=5))"
      ],
      "metadata": {
        "id": "lnNiWua8zV2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_job_requirements(jobreq):\n",
        "    translated_jobreq = {}\n",
        "\n",
        "    # Translate the entire job requirements string\n",
        "    translated_jobreq[\"job_requirements\"] = translate_text(jobreq)\n",
        "\n",
        "    return translated_jobreq"
      ],
      "metadata": {
        "id": "MeWF4-cnzYBg"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_jobreq = translate_job_requirements(jobreq)"
      ],
      "metadata": {
        "id": "gIzJZ2i1zaYp"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output the modified JSON for job requirements\n",
        "print(json.dumps(translate_jobreq, indent=4))"
      ],
      "metadata": {
        "id": "wBveJEpyzb6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Eudm7y2pzdgw"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install Sastrawi"
      ],
      "metadata": {
        "id": "SpG5Mh18zfJd"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "aOYVLEPDzjBz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1adf394-e598-44ff-c4b6-47d0b2cea781"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "english_stop_words = set(stopwords.words('english'))\n",
        "stopword_factory = StopWordRemoverFactory()\n",
        "indonesian_stop_words = set(stopword_factory.get_stop_words())\n",
        "all_stop_words = english_stop_words.union(indonesian_stop_words)\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "74d4xgWezkWJ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text):\n",
        "    if text is None:\n",
        "        return ''\n",
        "\n",
        "    if isinstance(text, dict):\n",
        "        text = preprocess_job_requirements(text)\n",
        "\n",
        "    text_no_punct = ''.join([char for char in text if char not in string.punctuation])\n",
        "    text_lower = text_no_punct.lower()\n",
        "    text_single_line = ' '.join(text_lower.split())\n",
        "    tokens = word_tokenize(text_single_line)\n",
        "    tokens_cleaned = [lemmatizer.lemmatize(word) for word in tokens if word not in all_stop_words]\n",
        "    clean_text = ' '.join(tokens_cleaned)\n",
        "\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "b2WCe0dFzmA7"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_job_requirements(jobreq):\n",
        "    # Preprocess the entire job requirements string\n",
        "    preprocessed_jobreq = text_preprocessing(jobreq)\n",
        "\n",
        "    return {\n",
        "        'job_requirements': preprocessed_jobreq\n",
        "    }"
      ],
      "metadata": {
        "id": "sVshi8Grznw_"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_jobreq = preprocess_job_requirements(translate_jobreq[\"job_requirements\"])"
      ],
      "metadata": {
        "id": "vMkIja9czpWk"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_jobreq"
      ],
      "metadata": {
        "id": "Al0KupsWzqwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_candidate(candidate):\n",
        "    work_experience = []\n",
        "    for experience in candidate.get('work_experience', []):\n",
        "        preprocessed_experience = {\n",
        "            'job_title': text_preprocessing(experience.get('job_title', '')),\n",
        "            'duration': text_preprocessing(experience.get('duration', ''))\n",
        "        }\n",
        "        work_experience.append(preprocessed_experience)\n",
        "\n",
        "    education = []\n",
        "    for edu in candidate.get('education', []):\n",
        "        if isinstance(edu, dict):\n",
        "            preprocessed_edu = {\n",
        "                'title': text_preprocessing(edu.get('title', '')),\n",
        "            }\n",
        "            education.append(preprocessed_edu)\n",
        "\n",
        "    languages = [text_preprocessing(language) if language != 'Indonesia' else language for language in candidate.get('languages', [])]\n",
        "    skills = [text_preprocessing(skill) for skill in candidate.get('skills', [])]\n",
        "\n",
        "    certification = []\n",
        "    for cert in candidate.get('certification', []):\n",
        "        if isinstance(cert, dict):\n",
        "            preprocessed_cert = {\n",
        "                'title': text_preprocessing(cert.get('title', '')),\n",
        "            }\n",
        "            certification.append(preprocessed_cert)\n",
        "\n",
        "    return {\n",
        "        'basic_info': candidate.get('basic_info', {}),\n",
        "        'work_experience': work_experience,\n",
        "        'education': education,\n",
        "        'languages': languages,\n",
        "        'skills': skills,\n",
        "        'certification': certification\n",
        "    }"
      ],
      "metadata": {
        "id": "7mTSadVnzsDt"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_candidates(candidates):\n",
        "    preprocessed_candidates = []\n",
        "    for candidate in candidates:\n",
        "        preprocessed_candidate = preprocess_candidate(candidate)\n",
        "        preprocessed_candidates.append(preprocessed_candidate)\n",
        "    return preprocessed_candidates\n"
      ],
      "metadata": {
        "id": "F9Hzovyuztv9"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_candidates = preprocess_candidates(candidates)"
      ],
      "metadata": {
        "id": "l_YWcZ00zvRf"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(json.dumps(clean_candidates, indent=4))"
      ],
      "metadata": {
        "id": "va3qBf1LzyG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data.json\", \"w\") as final:\n",
        "    json.dump(clean_candidates, final)"
      ],
      "metadata": {
        "id": "3ytojnW7PWjh"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('data.json')\n",
        "\n",
        "# returns JSON object as\n",
        "# a dictionary\n",
        "clean_candidates = json.load(f)"
      ],
      "metadata": {
        "id": "z5bXE0NjPFRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove stopwords and lemmatize\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Function to combine and preprocess candidate data\n",
        "def preprocess_candidate_data(candidate):\n",
        "    combined_text = []\n",
        "    # Combine basic info\n",
        "    basic_info = candidate['basic_info']\n",
        "    combined_text.append(basic_info['name'])\n",
        "    combined_text.append(basic_info['location'])\n",
        "\n",
        "    # Combine work experience\n",
        "    for work in candidate['work_experience']:\n",
        "        for i in range (10):\n",
        "          combined_text.append(work['job_title'])\n",
        "        if 'company' in work.keys():\n",
        "          combined_text.append(work['company'])\n",
        "        combined_text.append(work['location'])\n",
        "        combined_text.append(work['start_date'])\n",
        "        combined_text.append(work['end_date'])\n",
        "        if 'job_desc' in work.keys():\n",
        "          combined_text.extend(work['job_desc'])\n",
        "\n",
        "    # Combine education\n",
        "    for education in candidate['education']:\n",
        "        combined_text.append(education['title'])\n",
        "\n",
        "\n",
        "    # Combine languages, skills, and certifications\n",
        "    combined_text.extend(candidate['languages'])\n",
        "    combined_text.extend(candidate['skills'])\n",
        "    for certification in candidate['certification']:\n",
        "        for i in range (10):\n",
        "          combined_text.append(certification['title'])\n",
        "        combined_text.append(certification['issuer'])\n",
        "\n",
        "    # Join all text into a single string\n",
        "    combined_text = ' '.join(combined_text)\n",
        "    # Preprocess the combined text\n",
        "    return preprocess_text(combined_text)\n",
        "\n",
        "# Preprocess job requirements\n",
        "preprocessed_jobreq = preprocess_text(jobreq)"
      ],
      "metadata": {
        "id": "zTyWsD5DzzkO"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate and preprocess job requirement\n",
        "translated_jobreq = translate_text(jobreq)\n",
        "preprocessed_jobreq = preprocess_text(translated_jobreq)\n",
        "print(preprocessed_jobreq)"
      ],
      "metadata": {
        "id": "8Oggb9FYz1Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Translate and preprocess all candidates\n",
        "preprocessed_candidates = [preprocess_candidate_data(candidate) for candidate in candidates]\n",
        "\n",
        "# Print the preprocessed candidates\n",
        "for i, preprocessed_candidate in enumerate(preprocessed_candidates):\n",
        "    print(f\"Candidate {i+1}:\")\n",
        "    print(preprocessed_candidate)\n",
        "    print()"
      ],
      "metadata": {
        "id": "y5jPHXugz2rE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "QS5UZC-wz4IV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy jobreq_data DataFrame\n",
        "jobreq_data = pd.DataFrame({\n",
        "    'description': [preprocessed_jobreq]\n",
        "})\n",
        "\n",
        "# Assuming preprocessed_candidates is a list of preprocessed candidate strings\n",
        "cv_data = pd.DataFrame({\n",
        "    'Text': preprocessed_candidates\n",
        "})"
      ],
      "metadata": {
        "id": "AFm25fhSz54y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jobreq_data"
      ],
      "metadata": {
        "id": "RjSaqAZ-z7Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data"
      ],
      "metadata": {
        "id": "fvxoYJPpz8uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data.to_csv('data_train.csv')"
      ],
      "metadata": {
        "id": "4bg4_M0XMx_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = load_model('/content/collaborative_filtering_model (2).h5')"
      ],
      "metadata": {
        "id": "B59ey8rjz-G2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize TF-IDF Vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit the vectorizer on the CV data\n",
        "vectorizer.fit(cv_data['Text'])\n",
        "\n",
        "# Transform the job requirement description\n",
        "jobreq_tfidf = vectorizer.transform(jobreq_data['description'])\n",
        "\n",
        "# Define a function to recommend CVs for a job requirement using the trained collaborative filtering model\n",
        "def recommend_cvs_for_job(jobreq_id, model, vectorizer, jobreq_data, cv_data, top_n=5):\n",
        "    # Transform the job description to its TF-IDF representation\n",
        "    jobreq_tfidf = vectorizer.transform([jobreq_data.loc[jobreq_id, 'description']])\n",
        "    # Get the indices of CVs\n",
        "    cv_indices = np.arange(len(cv_data))\n",
        "    # Repeat the job requirement index for all CVs\n",
        "    job_indices = np.repeat(jobreq_id, len(cv_data))\n",
        "    # Predict the points using the model\n",
        "    predicted_points = model.predict([cv_indices, job_indices])\n",
        "    # Get the top recommended CVs\n",
        "    top_cvs_indices = predicted_points.squeeze().argsort()[::-1][:top_n]\n",
        "    # Get the CV texts and similarity scores\n",
        "    top_cvs_with_scores = []\n",
        "    for cv_idx in top_cvs_indices:\n",
        "        cv_text = cv_data.loc[cv_idx, 'Text']\n",
        "        similarity_score = predicted_points[cv_idx]\n",
        "        top_cvs_with_scores.append((cv_text, similarity_score))\n",
        "    return top_cvs_with_scores\n",
        "\n",
        "# Example: get top 5 CVs for a specific job requirement using the collaborative filtering model\n",
        "jobreq_id = 0  # Replace with a job requirement index for testing\n",
        "\n",
        "top_cvs_with_scores = recommend_cvs_for_job(jobreq_id, model, vectorizer, jobreq_data, cv_data)\n",
        "\n",
        "print(\"Top CVs for Job Requirement ID {}: \".format(jobreq_id))\n",
        "for cv_text, similarity_score in top_cvs_with_scores:\n",
        "    print(f\"CV: {cv_text}\\nSimilarity Score: {similarity_score}\\n\")\n",
        "\n",
        "# SAMPAI SINI AJA DULU YG BENER"
      ],
      "metadata": {
        "id": "2aZe8Xh50HfU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_data.to_csv('data_train.csv')"
      ],
      "metadata": {
        "id": "qIGzsyqo0I_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F7Unw_oN01et"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}