{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9768242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_recommenders as tfrs\n",
    "\n",
    "# Load the data\n",
    "jobreq_data = pd.read_csv(\"cleaned_vacancies_data.csv\")\n",
    "cv_data = pd.read_csv(\"cleaned_cv_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12df411e",
   "metadata": {},
   "source": [
    "Translate the datasets first to avoid biased points of similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e533657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "# Initialize the translator\n",
    "translator = Translator()\n",
    "\n",
    "# Function to detect and translate text\n",
    "def translate_text(text):\n",
    "    try:\n",
    "        # Detect the language\n",
    "        lang = detect(text)\n",
    "        # Translate only if the text is in Indonesian\n",
    "        if lang == 'id':\n",
    "            translated = translator.translate(text, src='id', dest='en')\n",
    "            return translated.text\n",
    "        else:\n",
    "            # Return the original text if it's not in Indonesian\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        # If detection or translation fails, return the original text\n",
    "        return text\n",
    "# Translate jobreq_data descriptions\n",
    "jobreq_data['description'] = jobreq_data['description'].apply(translate_text)\n",
    "\n",
    "# Translate cv_data text\n",
    "cv_data['Text'] = cv_data['Text'].apply(translate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e012e3",
   "metadata": {},
   "source": [
    "calculate the similarity of the jobreq (job requirement) dataset and the cv's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e845ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Concatenate all descriptions and texts to fit the vectorizer\n",
    "all_texts = pd.concat([jobreq_data['description'], cv_data['Text']])\n",
    "vectorizer.fit(all_texts)\n",
    "\n",
    "# Transform the job descriptions and CV texts to their TF-IDF representation\n",
    "jobreq_tfidf = vectorizer.transform(jobreq_data['description'])\n",
    "cv_tfidf = vectorizer.transform(cv_data['Text'])\n",
    "\n",
    "# Calculate cosine similarity using TensorFlow\n",
    "def cosine_similarity_tf(tfidf_matrix1, tfidf_matrix2):\n",
    "    tfidf_matrix1 = tf.constant(tfidf_matrix1.toarray(), dtype=tf.float32)\n",
    "    tfidf_matrix2 = tf.constant(tfidf_matrix2.toarray(), dtype=tf.float32)\n",
    "    dot_product = tf.linalg.matmul(tfidf_matrix1, tfidf_matrix2, transpose_b=True)\n",
    "    norm_matrix1 = tf.sqrt(tf.reduce_sum(tf.square(tfidf_matrix1), axis=1, keepdims=True))\n",
    "    norm_matrix2 = tf.sqrt(tf.reduce_sum(tf.square(tfidf_matrix2), axis=1, keepdims=True))\n",
    "    norm_product = tf.linalg.matmul(norm_matrix1, norm_matrix2, transpose_b=True)\n",
    "    similarity = dot_product / norm_product\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449d7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the similarity\n",
    "similarity_matrix = cosine_similarity_tf(jobreq_tfidf, cv_tfidf)\n",
    "similarity_matrix_np = similarity_matrix.numpy()\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix_np, index=jobreq_data.index, columns=cv_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c93408",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08f3479",
   "metadata": {},
   "source": [
    "Load the library for training the collaborative filtering model for candidate ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "195d62dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, Input, Dot, Flatten\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "043a8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the similarity matrix\n",
    "similarity_matrix_np = similarity_matrix.numpy()\n",
    "\n",
    "# Define function to prepare data for collaborative filtering\n",
    "def prepare_data(similarity_matrix):\n",
    "    # Get the indices of the top recommendations for each job\n",
    "    top_candidates_indices = np.argsort(similarity_matrix, axis=1)[:, ::-1]\n",
    "    # Create DataFrame to hold the user-item interactions\n",
    "    interactions = []\n",
    "    for job_idx, top_candidates in enumerate(top_candidates_indices):\n",
    "        for candidate_idx in top_candidates:\n",
    "            interactions.append([job_idx, candidate_idx, similarity_matrix[job_idx, candidate_idx]])\n",
    "    cv_data_with_points = pd.DataFrame(interactions, columns=['job_idx', 'candidate_idx', 'points'])\n",
    "    return cv_data_with_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4838d4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/128\n",
      "1662/1662 [==============================] - 4s 2ms/step - loss: 0.0015\n",
      "Epoch 2/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 2.5482e-04\n",
      "Epoch 3/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.8786e-04\n",
      "Epoch 4/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.6599e-04\n",
      "Epoch 5/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5935e-04\n",
      "Epoch 6/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5706e-04\n",
      "Epoch 7/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5554e-04\n",
      "Epoch 8/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5478e-04\n",
      "Epoch 9/128\n",
      "1662/1662 [==============================] - 4s 2ms/step - loss: 1.5466e-04\n",
      "Epoch 10/128\n",
      "1662/1662 [==============================] - 4s 2ms/step - loss: 1.5427e-04\n",
      "Epoch 11/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5349e-04\n",
      "Epoch 12/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5367e-04\n",
      "Epoch 13/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5301e-04\n",
      "Epoch 14/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5297e-04\n",
      "Epoch 15/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5329e-04\n",
      "Epoch 16/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5269e-04\n",
      "Epoch 17/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5297e-04\n",
      "Epoch 18/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5210e-04\n",
      "Epoch 19/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5282e-04\n",
      "Epoch 20/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5220e-04\n",
      "Epoch 21/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5223e-04\n",
      "Epoch 22/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5242e-04\n",
      "Epoch 23/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5199e-04\n",
      "Epoch 24/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5164e-04\n",
      "Epoch 25/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5198e-04\n",
      "Epoch 26/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5168e-04\n",
      "Epoch 27/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5160e-04\n",
      "Epoch 28/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5127e-04\n",
      "Epoch 29/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5116e-04\n",
      "Epoch 30/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5125e-04\n",
      "Epoch 31/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5123e-04\n",
      "Epoch 32/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5085e-04\n",
      "Epoch 33/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5095e-04\n",
      "Epoch 34/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5115e-04\n",
      "Epoch 35/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5078e-04\n",
      "Epoch 36/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5116e-04\n",
      "Epoch 37/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5007e-04\n",
      "Epoch 38/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5052e-04\n",
      "Epoch 39/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5017e-04\n",
      "Epoch 40/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5010e-04\n",
      "Epoch 41/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5029e-04\n",
      "Epoch 42/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4973e-04\n",
      "Epoch 43/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.5056e-04\n",
      "Epoch 44/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4993e-04\n",
      "Epoch 45/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4951e-04\n",
      "Epoch 46/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4942e-04\n",
      "Epoch 47/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4938e-04\n",
      "Epoch 48/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4948e-04\n",
      "Epoch 49/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4977e-04\n",
      "Epoch 50/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4948e-04\n",
      "Epoch 51/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4936e-04\n",
      "Epoch 52/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4883e-04\n",
      "Epoch 53/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4910e-04\n",
      "Epoch 54/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4900e-04\n",
      "Epoch 55/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4912e-04\n",
      "Epoch 56/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4886e-04\n",
      "Epoch 57/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4894e-04\n",
      "Epoch 58/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4877e-04\n",
      "Epoch 59/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4872e-04\n",
      "Epoch 60/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4893e-04\n",
      "Epoch 61/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4883e-04\n",
      "Epoch 62/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4861e-04\n",
      "Epoch 63/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4868e-04\n",
      "Epoch 64/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4863e-04\n",
      "Epoch 65/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4857e-04\n",
      "Epoch 66/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4849e-04\n",
      "Epoch 67/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4866e-04\n",
      "Epoch 68/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4861e-04\n",
      "Epoch 69/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4879e-04\n",
      "Epoch 70/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4816e-04\n",
      "Epoch 71/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4827e-04\n",
      "Epoch 72/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4873e-04\n",
      "Epoch 73/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4825e-04\n",
      "Epoch 74/128\n",
      "1662/1662 [==============================] - 4s 2ms/step - loss: 1.4840e-04\n",
      "Epoch 75/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4863e-04\n",
      "Epoch 76/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4840e-04\n",
      "Epoch 77/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4863e-04\n",
      "Epoch 78/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4821e-04\n",
      "Epoch 79/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4825e-04\n",
      "Epoch 80/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4849e-04\n",
      "Epoch 81/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4831e-04\n",
      "Epoch 82/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4836e-04\n",
      "Epoch 83/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4829e-04\n",
      "Epoch 84/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4848e-04\n",
      "Epoch 85/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4824e-04\n",
      "Epoch 86/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4808e-04\n",
      "Epoch 87/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4848e-04\n",
      "Epoch 88/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4833e-04\n",
      "Epoch 89/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4815e-04\n",
      "Epoch 90/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4819e-04\n",
      "Epoch 91/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4819e-04\n",
      "Epoch 92/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4823e-04\n",
      "Epoch 93/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4820e-04\n",
      "Epoch 94/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4822e-04\n",
      "Epoch 95/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4834e-04\n",
      "Epoch 96/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4819e-04\n",
      "Epoch 97/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4828e-04\n",
      "Epoch 98/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4822e-04\n",
      "Epoch 99/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4813e-04\n",
      "Epoch 100/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4824e-04\n",
      "Epoch 101/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4826e-04\n",
      "Epoch 102/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4826e-04\n",
      "Epoch 103/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4815e-04\n",
      "Epoch 104/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4811e-04\n",
      "Epoch 105/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4830e-04\n",
      "Epoch 106/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4814e-04\n",
      "Epoch 107/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4809e-04\n",
      "Epoch 108/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4805e-04\n",
      "Epoch 109/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4801e-04\n",
      "Epoch 110/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4836e-04\n",
      "Epoch 111/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4791e-04\n",
      "Epoch 112/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4827e-04\n",
      "Epoch 113/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4813e-04\n",
      "Epoch 114/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4793e-04\n",
      "Epoch 115/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4822e-04\n",
      "Epoch 116/128\n",
      "1662/1662 [==============================] - 4s 2ms/step - loss: 1.4803e-04\n",
      "Epoch 117/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4809e-04\n",
      "Epoch 118/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4786e-04\n",
      "Epoch 119/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4823e-04\n",
      "Epoch 120/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4792e-04\n",
      "Epoch 121/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4801e-04\n",
      "Epoch 122/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4779e-04\n",
      "Epoch 123/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4820e-04\n",
      "Epoch 124/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4813e-04\n",
      "Epoch 125/128\n",
      "1662/1662 [==============================] - 2s 1ms/step - loss: 1.4806e-04\n",
      "Epoch 126/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4799e-04\n",
      "Epoch 127/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4825e-04\n",
      "Epoch 128/128\n",
      "1662/1662 [==============================] - 3s 2ms/step - loss: 1.4840e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1748f047d90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare data for collaborative filtering\n",
    "cv_data_with_points = prepare_data(similarity_matrix_np)\n",
    "\n",
    "# Convert indices to continuous IDs for embedding purposes\n",
    "cv_data_with_points['job_idx'] = cv_data_with_points['job_idx'].astype('category').cat.codes\n",
    "cv_data_with_points['candidate_idx'] = cv_data_with_points['candidate_idx'].astype('category').cat.codes\n",
    "\n",
    "# Number of unique job requirements and candidates\n",
    "n_jobs = cv_data_with_points['job_idx'].nunique()\n",
    "n_candidates = cv_data_with_points['candidate_idx'].nunique()\n",
    "\n",
    "# Define the input layers\n",
    "user_input = Input(shape=(1,), name='user_input')\n",
    "item_input = Input(shape=(1,), name='item_input')\n",
    "\n",
    "# Define the embedding layers\n",
    "user_embedding = Embedding(n_candidates, 20, embeddings_initializer='uniform', name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(n_jobs, 20, embeddings_initializer='uniform', name='item_embedding')(item_input)\n",
    "\n",
    "# Flatten the embedding layers\n",
    "user_vecs = Flatten()(user_embedding)\n",
    "item_vecs = Flatten()(item_embedding)\n",
    "\n",
    "# Compute the dot product of the embeddings\n",
    "y = Dot(axes=1)([user_vecs, item_vecs])\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=[user_input, item_input], outputs=y)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Prepare the data for training\n",
    "X = cv_data_with_points[['candidate_idx', 'job_idx']].values\n",
    "y = cv_data_with_points['points'].values\n",
    "\n",
    "# Train the model\n",
    "model.fit([X[:, 0], X[:, 1]], y, epochs=128, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "003cb797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\anaconda3\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model to an .h5 file\n",
    "model.save('collaborative_filtering_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ba5867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "# Load the saved model\n",
    "model = load_model('collaborative_filtering_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6851483",
   "metadata": {},
   "source": [
    "Make a recommendation function to print the recommended candidate's cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94b36d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to recommend CVs for a job requirement using the trained collaborative filtering model\n",
    "def recommend_cvs_for_job(jobreq_id, model, vectorizer, jobreq_data, cv_data, top_n=5):\n",
    "    # Transform the job description to its TF-IDF representation\n",
    "    jobreq_tfidf = vectorizer.transform([jobreq_data.loc[jobreq_id, 'description']])\n",
    "    # Get the indices of CVs\n",
    "    cv_indices = np.arange(len(cv_data))\n",
    "    # Repeat the job requirement index for all CVs\n",
    "    job_indices = np.repeat(jobreq_id, len(cv_data))\n",
    "    # Predict the points using the model\n",
    "    predicted_points = model.predict([cv_indices, job_indices])\n",
    "    # Get the top recommended CVs\n",
    "    top_cvs_indices = predicted_points.squeeze().argsort()[::-1][:top_n]\n",
    "    # Get the CV texts and similarity scores\n",
    "    top_cvs_with_scores = []\n",
    "    for cv_idx in top_cvs_indices:\n",
    "        cv_text = cv_data.loc[cv_idx, 'Text']\n",
    "        similarity_score = predicted_points[cv_idx]\n",
    "        top_cvs_with_scores.append((cv_text, similarity_score))\n",
    "    return top_cvs_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579cf4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: get top 5 CVs for a specific job requirement using the collaborative filtering model\n",
    "jobreq_id = 0  # Replace with a job requirement index for testing\n",
    "\n",
    "# you can print the code below to check the regarding jobreq_text\n",
    "#jobreq_text = jobreq_data.iloc[jobreq_id]['description']\n",
    "#print(jobreq_text)\n",
    "top_cvs_with_scores = recommend_cvs_for_job(jobreq_id, model, vectorizer, jobreq_data, cv_data)\n",
    "\n",
    "print(\"Top CVs for Job Requirement ID {}: \".format(jobreq_id))\n",
    "for cv_text, similarity_score in top_cvs_with_scores:\n",
    "    print(f\"CV: {cv_text}\\nSimilarity Score: {similarity_score}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d70e434",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
