{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDBwI823ug8i"
   },
   "source": [
    "### Dummy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "CwVD9wtefXo4"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "EX1a3ERlbX-Y"
   },
   "outputs": [],
   "source": [
    "with open('data_train (2).json', 'r') as file:\n",
    "    candidates = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "0G6lY3NkhvWv"
   },
   "outputs": [],
   "source": [
    "jobreq = \"deskripsi kerja posisi lowong partisipasi prose bangun aplikasi sedang bangun usaha nama usaha harap mampu kerja tim responsibility buat modifikasi program tanggung jawab kelola program\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23O3Z1lwf1uR",
    "outputId": "8e936187-ab85-4dec-f0ab-575818a47221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q googletrans==4.0.0-rc1\n",
    "!pip install -q googletrans==4.0.0-rc1 langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "ReZxNHjzfuuo"
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "LKUZ1UyYflYQ"
   },
   "outputs": [],
   "source": [
    "def translate_text(text):\n",
    "    translator = Translator()\n",
    "    try:\n",
    "        # Detect the language\n",
    "        lang = detect(text)\n",
    "        # Translate only if the text is in Indonesian\n",
    "        if lang == 'id':\n",
    "            translated = translator.translate(text, src='id', dest='en')\n",
    "            return translated.text\n",
    "        else:\n",
    "            # Return the original text if it's not in Indonesian\n",
    "            return text\n",
    "    except Exception as e:\n",
    "        # If detection or translation fails, return the original text\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "VbUXKbg2yOdp"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "Uf-20Cf3vMlf"
   },
   "outputs": [],
   "source": [
    "def calculate_duration(start_date, end_date):\n",
    "    if start_date.lower() in ['none', 'unknown']:\n",
    "        return \"Unknown duration\"\n",
    "\n",
    "    # Try parsing the start_date in different formats\n",
    "    try:\n",
    "        start_date_obj = datetime.strptime(start_date, \"%b %Y\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            start_date_obj = datetime.strptime(start_date, \"%Y\")\n",
    "        except ValueError:\n",
    "            return \"Unknown duration\"\n",
    "\n",
    "    if end_date.lower() == 'present':\n",
    "        end_date_obj = datetime.now()\n",
    "    elif end_date.lower() in ['none', 'unknown']:\n",
    "        return \"Unknown duration\"\n",
    "    else:\n",
    "        # Try parsing the end_date in different formats\n",
    "        try:\n",
    "            end_date_obj = datetime.strptime(end_date, \"%b %Y\")\n",
    "        except ValueError:\n",
    "            try:\n",
    "                end_date_obj = datetime.strptime(end_date, \"%Y\")\n",
    "            except ValueError:\n",
    "                return \"Unknown duration\"\n",
    "\n",
    "    # Calculate the difference between end_date and start_date\n",
    "    duration = end_date_obj - start_date_obj\n",
    "\n",
    "    # Extract years and months from the duration\n",
    "    years = duration.days // 365\n",
    "    months = (duration.days % 365) // 30\n",
    "\n",
    "    # Format the duration\n",
    "    if years >= 1:\n",
    "        if years == 1:\n",
    "            return \"+1 year\"\n",
    "        elif years == 2:\n",
    "            return \"+2 years\"\n",
    "        else:\n",
    "            return f\"+{years} years\"\n",
    "    else:\n",
    "        return f\"{months} months\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "XgP6Irocxn5I"
   },
   "outputs": [],
   "source": [
    "# Function to translate candidates' data\n",
    "def translate_candidate_data(candidate):\n",
    "    # Translate work experience descriptions\n",
    "    for experience in candidate.get(\"work_experience\", []):\n",
    "        # Translate job title\n",
    "        job_title = experience.get(\"job_title\", \"\")\n",
    "        if job_title:\n",
    "            translated_title = translate_text(\"\".join(job_title))\n",
    "            experience[\"job_title\"] = translated_title\n",
    "\n",
    "        # Translate job description\n",
    "        if experience.get(\"job_desc\"):\n",
    "            experience[\"job_desc\"] = [translate_text(desc) for desc in experience[\"job_desc\"]]\n",
    "\n",
    "        # Calculate duration\n",
    "        duration = calculate_duration(experience.get(\"start_date\", \"\"), experience.get(\"end_date\", \"\"))\n",
    "        experience[\"duration\"] = duration\n",
    "\n",
    "    # Translate education title and description if they exist\n",
    "    for education in candidate.get(\"education\", []):\n",
    "        if isinstance(education, dict):\n",
    "            # Translate education title\n",
    "            education_title = education.get(\"title\", \"\")\n",
    "            if education_title:\n",
    "                translated_education_title = translate_text(\"\".join(education_title))\n",
    "                education[\"title\"] = translated_education_title\n",
    "\n",
    "            # Translate education description\n",
    "            if education.get(\"description\"):\n",
    "                education[\"description\"] = translate_text(education[\"description\"])\n",
    "\n",
    "    # Translate languages\n",
    "    candidate[\"languages\"] = [translate_text(language) for language in candidate.get(\"languages\", [])]\n",
    "\n",
    "    # Translate skills\n",
    "    candidate[\"skills\"] = [translate_text(skill) for skill in candidate.get(\"skills\", [])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "eAPYhwkWt8sk"
   },
   "outputs": [],
   "source": [
    "for candidate in candidates:\n",
    "    translate_candidate_data(candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N7XyjLkCkGRr",
    "outputId": "2edf969c-91c4-4f57-a459-cd1dbd4b91b2"
   },
   "outputs": [],
   "source": [
    "# Output the modified JSON for candidates\n",
    "print(json.dumps(candidates, indent=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "BYQBTIU4uFpj"
   },
   "outputs": [],
   "source": [
    "def translate_job_requirements(jobreq):\n",
    "    translated_jobreq = {}\n",
    "\n",
    "    # Translate the entire job requirements string\n",
    "    translated_jobreq[\"job_requirements\"] = translate_text(jobreq)\n",
    "\n",
    "    return translated_jobreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "i-kNvuXZuTiN"
   },
   "outputs": [],
   "source": [
    "translate_jobreq = translate_job_requirements(jobreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JxY-0ptHkG97",
    "outputId": "a42eb43b-9d19-4085-a463-bfc612d7eb67"
   },
   "outputs": [],
   "source": [
    "# Output the modified JSON for job requirements\n",
    "print(json.dumps(translate_jobreq, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "VTe_nqk34Bi7"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D2oXKuYy6TsS",
    "outputId": "3b08f3a1-fa85-492e-fab7-1cb975be88d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install Sastrawi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wtd4iuSu6aRO",
    "outputId": "d13e60c5-8b08-4b21-aa1f-996a3dd1a0c1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "english_stop_words = set(stopwords.words('english'))\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "indonesian_stop_words = set(stopword_factory.get_stop_words())\n",
    "all_stop_words = english_stop_words.union(indonesian_stop_words)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "FOBZ0aVz6eM2"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    if text is None:\n",
    "        return ''\n",
    "\n",
    "    if isinstance(text, dict):\n",
    "        text = preprocess_job_requirements(text)\n",
    "\n",
    "    text_no_punct = ''.join([char for char in text if char not in string.punctuation])\n",
    "    text_lower = text_no_punct.lower()\n",
    "    text_single_line = ' '.join(text_lower.split())\n",
    "    tokens = word_tokenize(text_single_line)\n",
    "    tokens_cleaned = [lemmatizer.lemmatize(word) for word in tokens if word not in all_stop_words]\n",
    "    clean_text = ' '.join(tokens_cleaned)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "B8o2SAsOFTat"
   },
   "outputs": [],
   "source": [
    "def preprocess_job_requirements(jobreq):\n",
    "    # Preprocess the entire job requirements string\n",
    "    preprocessed_jobreq = text_preprocessing(jobreq)\n",
    "\n",
    "    return {\n",
    "        'job_requirements': preprocessed_jobreq\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "bbYXf7MyD2_N"
   },
   "outputs": [],
   "source": [
    "clean_jobreq = preprocess_job_requirements(translate_jobreq[\"job_requirements\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mh-cxOAKqNWY",
    "outputId": "998c11bc-3250-46c1-d964-06d90effec12"
   },
   "outputs": [],
   "source": [
    "clean_jobreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "uW4K6OEdG_Mx"
   },
   "outputs": [],
   "source": [
    "def preprocess_candidate(candidate):\n",
    "    work_experience = []\n",
    "    for experience in candidate.get('work_experience', []):\n",
    "        preprocessed_experience = {\n",
    "            'job_title': text_preprocessing(experience.get('job_title', '')),\n",
    "            'company': text_preprocessing(experience.get('company', '')),\n",
    "            'location': text_preprocessing(experience.get('location', '')),\n",
    "            'start_date': text_preprocessing(experience.get('start_date', '')),\n",
    "            'end_date': text_preprocessing(experience.get('end_date', '')),\n",
    "            'job_desc': [text_preprocessing(desc) for desc in experience.get('job_desc', [])],\n",
    "            'duration': text_preprocessing(experience.get('duration', ''))\n",
    "        }\n",
    "        work_experience.append(preprocessed_experience)\n",
    "\n",
    "    education = []\n",
    "    for edu in candidate.get('education', []):\n",
    "        if isinstance(edu, dict):\n",
    "            preprocessed_edu = {\n",
    "                'title': text_preprocessing(edu.get('title', '')),\n",
    "                'institute': text_preprocessing(edu.get('institute', '')),\n",
    "                'location': text_preprocessing(edu.get('location', '')),\n",
    "                'start_date': text_preprocessing(edu.get('start_date', '')),\n",
    "                'end_date': text_preprocessing(edu.get('end_date', '')),\n",
    "                'description': text_preprocessing(edu.get('description', ''))\n",
    "            }\n",
    "            education.append(preprocessed_edu)\n",
    "\n",
    "    languages = [text_preprocessing(language) if language != 'Indonesia' else language for language in candidate.get('languages', [])]\n",
    "    skills = [text_preprocessing(skill) for skill in candidate.get('skills', [])]\n",
    "\n",
    "    certification = []\n",
    "    for cert in candidate.get('certification', []):\n",
    "        if isinstance(cert, dict):\n",
    "            preprocessed_cert = {\n",
    "                'title': text_preprocessing(cert.get('title', '')),\n",
    "                'issuer': text_preprocessing(cert.get('issuer', '')),\n",
    "                'start_date': text_preprocessing(cert.get('start_date', '')),\n",
    "                'expiration_date': text_preprocessing(cert.get('expiration_date', ''))\n",
    "            }\n",
    "            certification.append(preprocessed_cert)\n",
    "\n",
    "    return {\n",
    "        'basic_info': candidate.get('basic_info', {}),\n",
    "        'work_experience': work_experience,\n",
    "        'education': education,\n",
    "        'languages': languages,\n",
    "        'skills': skills,\n",
    "        'certification': certification\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "8AgpwQlBBUXj"
   },
   "outputs": [],
   "source": [
    "def preprocess_candidates(candidates):\n",
    "    preprocessed_candidates = []\n",
    "    for candidate in candidates:\n",
    "        preprocessed_candidate = preprocess_candidate(candidate)\n",
    "        preprocessed_candidates.append(preprocessed_candidate)\n",
    "    return preprocessed_candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "JOpthyrEHAFs"
   },
   "outputs": [],
   "source": [
    "clean_candidates = preprocess_candidates(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0G60QlKDx3vQ",
    "outputId": "c2facc75-4431-49ac-8de8-ff81b468f553"
   },
   "outputs": [],
   "source": [
    "print(json.dumps(clean_candidates, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = text.replace('unknown', '')\n",
    "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "def preprocess_candidate_data(candidate):\n",
    "    combined_text = []\n",
    "    \n",
    "    # Combine basic info\n",
    "    basic_info = candidate.get('basic_info', {})\n",
    "    combined_text.append(basic_info.get('location', ''))\n",
    "\n",
    "    # Combine work experience\n",
    "    for work in candidate.get('work_experience', []):\n",
    "        if isinstance(work, dict):\n",
    "            combined_text.append(work.get('job_title', ''))\n",
    "            combined_text.append(work.get('company', ''))\n",
    "            combined_text.append(work.get('location', ''))\n",
    "            combined_text.append(' '.join(work.get('job_desc', [])))\n",
    "\n",
    "    # Combine education\n",
    "    for education in candidate.get('education', []):\n",
    "        if isinstance(education, dict):\n",
    "            combined_text.append(education.get('title', ''))\n",
    "            combined_text.append(education.get('institute', ''))\n",
    "            combined_text.append(education.get('location', ''))\n",
    "            combined_text.append(education.get('description', ''))\n",
    "\n",
    "    # Combine languages, skills, and certifications\n",
    "    combined_text.extend(candidate.get('languages', []))\n",
    "    combined_text.extend(candidate.get('skills', []))\n",
    "    for certification in candidate.get('certification', []):\n",
    "        if isinstance(certification, dict):\n",
    "            combined_text.append(certification.get('title', ''))\n",
    "            combined_text.append(certification.get('issuer', ''))\n",
    "\n",
    "    # Ensure all elements in combined_text are strings\n",
    "    combined_text = ' '.join(map(str, combined_text))\n",
    "    return preprocess_text(combined_text)\n",
    "\n",
    "# Preprocess job requirements\n",
    "preprocessed_jobreq = preprocess_text(jobreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate and preprocess job requirement\n",
    "translated_jobreq = translate_text(jobreq)\n",
    "preprocessed_jobreq = preprocess_text(translated_jobreq)\n",
    "print(preprocessed_jobreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate and preprocess all candidates\n",
    "for candidate in candidates:\n",
    "    translate_candidate_data(candidate)\n",
    "preprocessed_candidates = [preprocess_candidate_data(candidate) for candidate in candidates]\n",
    "\n",
    "# Print the preprocessed candidates\n",
    "for i, preprocessed_candidate in enumerate(preprocessed_candidates):\n",
    "    print(f\"Candidate {i+1}:\")\n",
    "    print(preprocessed_candidate)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dummy jobreq_data DataFrame\n",
    "jobreq_data = pd.DataFrame({\n",
    "    'description': [preprocessed_jobreq]\n",
    "})\n",
    "\n",
    "# Assuming preprocessed_candidates is a list of preprocessed candidate strings\n",
    "cv_data = pd.DataFrame({\n",
    "    'Text': preprocessed_candidates\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobreq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model = load_model('collaborative_filtering_model_newest.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the CV data\n",
    "vectorizer.fit(cv_data['Text'])\n",
    "\n",
    "# Transform the job requirement description\n",
    "jobreq_tfidf = vectorizer.transform(jobreq_data['description'])\n",
    "\n",
    "# Define a function to recommend CVs for a job requirement using the trained collaborative filtering model\n",
    "def recommend_cvs_for_job(jobreq_id, model, vectorizer, jobreq_data, cv_data, top_n=5):\n",
    "    # Transform the job description to its TF-IDF representation\n",
    "    jobreq_tfidf = vectorizer.transform([jobreq_data.loc[jobreq_id, 'description']])\n",
    "    \n",
    "    # Get the indices of CVs\n",
    "    cv_indices = np.arange(len(cv_data))\n",
    "    \n",
    "    # Repeat the job requirement index for all CVs\n",
    "    job_indices = np.repeat(jobreq_id, len(cv_data))\n",
    "    \n",
    "    # Predict the relevance scores using the model\n",
    "    predicted_points = model.predict([cv_indices, job_indices])\n",
    "    \n",
    "    # Get the top recommended CVs\n",
    "    top_cvs_indices = predicted_points.squeeze().argsort()[::-1][:top_n]\n",
    "    \n",
    "    # Get the CV indices (acting as CV IDs), texts, and similarity scores\n",
    "    top_cvs_with_scores = []\n",
    "    for cv_idx in top_cvs_indices:\n",
    "        cv_id = cv_idx  # Use the row index as the CV ID\n",
    "        cv_text = cv_data.loc[cv_idx, 'Text']\n",
    "        similarity_score = predicted_points[cv_idx].item()  # Convert to a scalar value\n",
    "        top_cvs_with_scores.append((cv_id, cv_text, similarity_score))\n",
    "    \n",
    "    return top_cvs_with_scores\n",
    "\n",
    "# Example: get top 5 CVs for a specific job requirement using the collaborative filtering model\n",
    "jobreq_id = 0  # Replace with the appropriate job requirement index\n",
    "\n",
    "top_cvs_with_scores = recommend_cvs_for_job(jobreq_id, model, vectorizer, jobreq_data, cv_data)\n",
    "\n",
    "print(\"Top CVs for Job Requirement ID {}: \".format(jobreq_id))\n",
    "for cv_id, cv_text, similarity_score in top_cvs_with_scores:\n",
    "    print(f\"CV ID: {cv_id}\\nCV: {cv_text}\\nSimilarity Score: {similarity_score}\\n\")\n",
    "\n",
    "# Store the results in a dictionary\n",
    "result_dict = {\n",
    "    \"jobreq_id\": jobreq_id,\n",
    "    \"recommendations\": [\n",
    "        {\n",
    "            \"cv_id\": cv_id,\n",
    "            \"cv_text\": cv_text,\n",
    "            \"similarity_score\": similarity_score\n",
    "        }\n",
    "        for cv_id, cv_text, similarity_score in top_cvs_with_scores\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Write the dictionary to a JSON file\n",
    "output_file = 'recommended_cvs.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(result_dict, f, indent=4)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
